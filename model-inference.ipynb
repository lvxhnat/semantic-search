{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cae9ef66-36df-4254-92d8-a785ba6ecbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lohyikuang/miniconda3/envs/change-driver-library/lib/python3.11/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from mlengine.core.model import train_mlm\n",
    "\n",
    "def load_json_dataset(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def flatten_entry(data_entry):\n",
    "    result = []\n",
    "    for data in data_entry:\n",
    "        what = data['what']\n",
    "        if what.strip() != \"\":\n",
    "            result.append(what)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a0019c6-e234-4de9-b178-8a66cd473f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "eclipse_dataset = load_json_dataset(\"data/msr2013-bug_dataset/data/v02/eclipse/short_desc.json\")\n",
    "mozilla_dataset = load_json_dataset(\"data/msr2013-bug_dataset/data/v02/mozilla/short_desc.json\")\n",
    "\n",
    "c_mozilla_dataset = [*itertools.chain.from_iterable(map(flatten_entry, mozilla_dataset['short_desc'].values()))]\n",
    "c_eclipse_dataset = [*itertools.chain.from_iterable(map(flatten_entry, eclipse_dataset['short_desc'].values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c964df8e-a206-4432-825e-fe439c1a7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(set(c_mozilla_dataset + c_eclipse_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16c50ba8-5961-457a-94d3-75afab4c138a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This URL (java) opens in the Sidebar instead of the browser window',\n",
       " 'ToolItem remains pressed',\n",
       " 'New bookmarks lost when closing Firefox',\n",
       " '[PostgreSQL] ./collectstats.pl --regenerate fails (again)',\n",
       " 'JavaScript code bug in pref-crlupdate.js',\n",
       " 'Program errors and closes.',\n",
       " '[CommonNavigator] Common Navigator Array Store Exception',\n",
       " '[Keybindings] Ctrl Shift T not working on Linux GTK',\n",
       " 'layout gets fouled up when serving from cache.',\n",
       " 'bmp image served from router fails to reload despite Cache-control: no-cache directive',\n",
       " 'let me link to about:logo',\n",
       " 'Provide more control for drag source to influence DnD operation',\n",
       " 'Job#join should allow passing a progress monitor',\n",
       " '[Oracle] collectstats.pl --regenerate uses PL/SQL reserved word in query',\n",
       " 'Javadoc warnings in N20070129-0010',\n",
       " 'Packages missing from open type dialog (1GL5001)',\n",
       " 'netscape stock quote page lays out badly',\n",
       " '[Progress] remove deprecated IProgressConstants.COMMAND_PROPERTY',\n",
       " 'print hangs',\n",
       " 'Progress during plugin installation is modal',\n",
       " 'eval, in javacript, when executed, causes a GET/POST request to the page being currently viewed',\n",
       " 'Ship a separate Toolbar.png optimized for Personas',\n",
       " 'indexing help takes a long time',\n",
       " 'On bug creation, the first confirmed bug status should be selected by default for users with canconfirm privs, even if UNCONFIRMED is the bug status with the lowest sortkey',\n",
       " '[hovering] Java hover does not show up on constructor references']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "254517e9-bd86-4f70-ae6f-879a5a940ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'{os.path.abspath(\"\")}/train.txt'\n",
    "with open(file_path, 'w') as f:\n",
    "    for line in dataset[:10]:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb0a990d-5928-415e-a15c-4e3f0f228b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sentence-transformers/all-mpnet-base-v2 were not used when initializing MPNetForMaskedLM: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing MPNetForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MPNetForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MPNetForMaskedLM were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save checkpoints to: output/sentence-transformers_all-mpnet-base-v2-2024-06-09_19-15-36\n",
      "Train sentences: 10\n",
      "Save tokenizer to: output/sentence-transformers_all-mpnet-base-v2-2024-06-09_19-15-36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lohyikuang/miniconda3/envs/change-driver-library/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/lohyikuang/miniconda3/envs/change-driver-library/lib/python3.11/site-packages/transformers/data/data_collator.py:1019: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to: output/sentence-transformers_all-mpnet-base-v2-2024-06-09_19-15-36\n",
      "Training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output/sentence-transformers_all-mpnet-base-v2-2024-06-09_19-15-36'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "train_mlm(\"sentence-transformers/all-mpnet-base-v2\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23c2decc-dc57-485d-8234-18dcea72eff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "saved_model_path = f'{os.path.abspath(\"\")}/output/sentence-transformers_all-mpnet-base-v2-2024-06-09_18-09-14/'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model_path)\n",
    "model = AutoModelForMaskedLM.from_pretrained(saved_model_path, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f87b0914-103d-4a8f-909d-fb4848694ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [01:21<00:00, 11.05it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = dataset[100:1000]\n",
    "corpus_embeddings = []\n",
    "for doc in tqdm(corpus):\n",
    "    encoded_input = tokenizer(doc, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "        embeddings = output.hidden_states[-1].mean(dim=1)  # Using mean pooling\n",
    "    corpus_embeddings.append(embeddings)\n",
    "corpus_embeddings = torch.cat(corpus_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47494ece-f293-4964-868b-5f2a364eb81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([900, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9688d302-2c95-4596-b52b-8980da9e28fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "k = 5\n",
    "query = \"reboot\"# dataset[random.randint(1000, len(dataset))]\n",
    "\n",
    "# Tokenize and encode the query into an embedding\n",
    "encoded_query = tokenizer(query, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    output = model(**encoded_query)\n",
    "    query_embedding = output.hidden_states[-1].mean(dim=1)  # Using mean pooling\n",
    "\n",
    "similarities = cosine_similarity(query_embedding, corpus_embeddings)\n",
    "top_k_indices = np.argsort(similarities[0])[-k:][::-1]\n",
    "top_k_documents = [corpus[i] for i in top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee0fa9a2-30cd-4f02-bc3e-d28b7a280df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inconsistency: it says click refresh button or do Ctrl+Alt+R, these are different',\n",
       " 'Firebird fails to start',\n",
       " \"I can enter my user name on the pop-up but not my password, so I can't get onto this page\",\n",
       " 'page does not load correctly',\n",
       " 'could not start eclipse']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
