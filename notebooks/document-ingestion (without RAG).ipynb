{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19115cc9-98f3-40c8-8134-0ae88e182afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yikuang/miniconda3/envs/semantic-search/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import uuid\n",
    "import chromadb\n",
    "import tempfile\n",
    "from dotenv import load_dotenv\n",
    "from pdf2image import convert_from_path\n",
    "import torch\n",
    "from typing import List\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer, pipeline, QuantoConfig, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0347932-3823-4e2e-b5e7-b67ca32008cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiology_sample = \"Radiology_example-chest_report.pdf\"\n",
    "physiotherapy_sample = \"Physical Therapy Progress Note.pdf\"\n",
    "physexam_sample = \"Physical_Exam_Sample.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d75064c-304c-4baf-a5f8-7abbd7ef735b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 29s, sys: 2.07 s, total: 1min 31s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ocr_model_id = 'ucaslcl/GOT-OCR2_0'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ocr_model_id, trust_remote_code=True)\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    ocr_model_id, \n",
    "    trust_remote_code=True, \n",
    "    low_cpu_mem_usage=True, \n",
    "    device_map='cuda', \n",
    "    use_safetensors=True, \n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    quantization_config=QuantoConfig(weights=\"int8\"),\n",
    ")\n",
    "model = model.eval().cuda()\n",
    "\n",
    "file_path = f\"./data/pdf-samples/{physexam_sample}\"\n",
    "\n",
    "images = convert_from_path(file_path)\n",
    "\n",
    "pdf_data = []\n",
    "for image in images:\n",
    "    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_file:\n",
    "        # Save your image to this temporary file\n",
    "        temp_file_path = temp_file.name\n",
    "        image.save(temp_file_path)\n",
    "        res = model.chat(tokenizer, temp_file_path, ocr_type='format')\n",
    "        pdf_data.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf4fa9e-677b-40e3-a733-7bb12c0032f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64e909e3-e739-4c64-b145-4184ac7c3ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_pdf = \" \".join(pdf_data)\n",
    "document_content = \"\\n\".join(pdf_data)\n",
    "document_content = re.sub(\n",
    "    r'\\\\\\((.*?)\\\\\\)',\n",
    "    '', \n",
    "    document_content.replace(\"\\n\", \" \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bea4dc9d-fe24-44e9-8779-059f8da72f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "\n",
    "torch.random.manual_seed(0) \n",
    "\n",
    "class InitiateLLM:\n",
    "    \n",
    "    def __init__(self, context: str, history_enabled: bool = False):\n",
    "\n",
    "        self.context = context\n",
    "        self.history_enabled = history_enabled\n",
    "        self.model_id = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "        self.generation_args = { \n",
    "            \"max_new_tokens\": 500, \n",
    "            \"return_full_text\": False, \n",
    "            \"temperature\": 0.0, \n",
    "            \"do_sample\": False, \n",
    "        } \n",
    "\n",
    "        # Load model with quantization for reduced memory usage\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_id,\n",
    "            device_map=\"auto\", \n",
    "            trust_remote_code=False,\n",
    "            quantization_config=QuantoConfig(weights=\"float8\"),\n",
    "        )\n",
    "        \n",
    "        # Enable gradient checkpointing to reduce memory footprint\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "        \n",
    "        # Define the pipeline\n",
    "        self.pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "        self.prompt_acc = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "        ]\n",
    "\n",
    "    def _create_query(self, query: str, context: str):\n",
    "        return {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": (\n",
    "                \"Answer the following question using only the provided context. Do not assume or add information beyond what is in the context. \\n\"\n",
    "                \"If the context does not contain sufficient information to answer the question or no context is provided at all, explicitly state that the context is insufficient. \\n\"\n",
    "                \"In your own understanding, if the description of the issue is vague, ask for clarifications \\n\"\n",
    "                f'Question: {query}'\n",
    "                f'Context: {context}'\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def prompt(self, query: str):\n",
    "\n",
    "        generated_prompt = self._create_query(query, self.context)\n",
    "\n",
    "        if self.history_enabled:\n",
    "            self.prompt_acc.append(generated_prompt)\n",
    "            with torch.no_grad():\n",
    "                output = self.pipe(self.prompt_acc, **self.generation_args)\n",
    "                output = output[0]['generated_text']\n",
    "            self.prompt_acc.append({ \"role\": \"system\", \"content\": output })\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                prompt_ = self.prompt_acc + [generated_prompt]\n",
    "                output = self.pipe(prompt_, **self.generation_args)\n",
    "                output = output[0]['generated_text']\n",
    "                \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d3b6c25-7c87-407f-9d32-42c00d0cb9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████| 2/2 [00:03<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "llm = InitiateLLM(\"TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4aa73272-c1b5-46b6-bf06-745ac9dd4318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.21 s, sys: 1.74 ms, total: 6.21 s\n",
      "Wall time: 6.21 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' I\\'m sorry, but the context provided is insufficient to answer any question regarding the patient\\'s name. The context only contains the word \"TEST,\" which does not provide any information about a patient\\'s name.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.prompt(\"what is the patient_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0661c41b-1938-4056-bdea-d3f5be65eba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|████████████████████████| 2/2 [00:01<00:00,  1.92it/s]\n",
      "/home/yikuang/miniconda3/envs/semantic-search/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To solve the equation 2x + 3 = 7, follow these steps:\n",
      "\n",
      "1. Subtract 3 from both sides of the equation:\n",
      "   2x + 3 - 3 = 7 - 3\n",
      "   2x = 4\n",
      "\n",
      "2. Divide both sides of the equation by 2:\n",
      "   2x/2 = 4/2\n",
      "   x = 2\n",
      "\n",
      "So, the solution to the equation 2x + 3 = 7 is x = 2.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "\n",
    "torch.random.manual_seed(0) \n",
    "model = AutoModelForCausalLM.from_pretrained( \n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",  \n",
    "    device_map=\"cuda\",  \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,  \n",
    ") \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\") \n",
    "\n",
    "messages = [ \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"}, \n",
    "] \n",
    "\n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ") \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"return_full_text\": False, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "output = pipe(messages, **generation_args) \n",
    "print(output[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-search",
   "language": "python",
   "name": "semantic-search"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
