{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aad3ce6-66f9-4191-a486-476a146f54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def request_issue(\n",
    "    repo_link: str = \"https://github.com/pandas-dev/pandas.git\",\n",
    "    page_number: int = 1\n",
    "):\n",
    "    \n",
    "    repo_identifier = \"/\".join(repo_link.split(\"/\")[-2:]).replace(\".git\", \"\")\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': 'token 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',\n",
    "        'Accept': 'application/vnd.github.v3+json'\n",
    "    }\n",
    "    \n",
    "    params = {\n",
    "        \"state\": \"all\", \n",
    "        \"since\": \"2011-01-01T00:00:00Z\",\n",
    "        \"per_page\": 100,\n",
    "        \"page\": page_number\n",
    "    }\n",
    "\n",
    "    data = requests.get(\n",
    "        f\"https://api.github.com/repos/{repo_identifier}/issues\", \n",
    "        params = params, \n",
    "        headers = headers\n",
    "    ).json()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def serialise_github_issues(d):\n",
    "    data = {}\n",
    "    for field in fields: \n",
    "        item = d[field]\n",
    "        if field == 'labels':\n",
    "            item = [entry['name'] for entry in item]\n",
    "        if field == 'user': \n",
    "            item = item['login']\n",
    "        data[field] = str(item)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57986e35-f136-4d9d-b0f6-dea8895a8240",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting for pytorch\n",
      "Extracting 100 pages for 100\n",
      "Extracting 100 pages for 200\n",
      "Extracting 100 pages for 300\n",
      "Extracting 100 pages for 400\n",
      "Extracting 100 pages for 500\n",
      "Extracting 100 pages for 600\n",
      "Extracting 100 pages for 700\n",
      "Extracting 100 pages for 800\n",
      "Extracting 100 pages for 900\n",
      "Extracting 100 pages for 1000\n",
      "Extracting 100 pages for 1100\n",
      "Extracting 100 pages for 1200\n",
      "Extracting 100 pages for 1300\n",
      "Extracting 100 pages for 1400\n",
      "Extracting 100 pages for 1500\n",
      "Extracting 100 pages for 1600\n",
      "Extracting 100 pages for 1700\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "labels = [\n",
    "#    \"tensorflow\",\n",
    "    \"pytorch\",\n",
    "    \"numpy\",\n",
    "    \"scipy\",\n",
    "]\n",
    "\n",
    "repo_links = [\n",
    "#    \"https://github.com/tensorflow/tensorflow.git\",\n",
    "    \"https://github.com/pytorch/pytorch\",\n",
    "    \"https://github.com/numpy/numpy\",\n",
    "    \"https://github.com/scipy/scipy\",\n",
    "]\n",
    "\n",
    "for label, repo_link in zip(labels, repo_links):\n",
    "    \n",
    "    print(f\"Extracting for {label}\")\n",
    "    \n",
    "    filename = f\"{datetime.now().strftime('%Y%m%d%H%M')}-{label}\"\n",
    "    \n",
    "    try: \n",
    "        d = request_issue(repo_link)\n",
    "\n",
    "\n",
    "        # Scrape github\n",
    "        i = 1\n",
    "\n",
    "        while len(d) % 100 == 0:\n",
    "\n",
    "            i += 1\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Extracting 100 pages for {i}\")\n",
    "\n",
    "            d += request_issue(repo_link, page_number = i)\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "\n",
    "        with open(f\"./data/raw-{filename}.json\", \"w\") as f:\n",
    "            json.dump(d, f)\n",
    "\n",
    "        fields = [\n",
    "            'url', 'repository_url', 'id', 'user', 'node_id', 'title', 'state', \n",
    "            'labels', 'created_at', 'updated_at', 'body'\n",
    "        ]\n",
    "\n",
    "        serialised_data = [serialise_github_issues(i_) for i_ in d]\n",
    "        pd.DataFrame(serialised_data).to_csv(f\"./data/{filename}.csv\", index = False)\n",
    "        \n",
    "    except Exception as e: \n",
    "        \n",
    "        print(f\"Error for page {i}\") \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6312cfff-e54c-4804-8ae3-86f8f7c2a972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71850"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5d91a311-1567-48c8-94cc-d10e3534e95e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stochastic-differentials",
   "language": "python",
   "name": "stochastic-differentials"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
