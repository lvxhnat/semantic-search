{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a42c120-c00e-4598-a15b-9914243c9abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yikuang/miniconda3/envs/rapids-defectsearch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "\n",
    "hf_token = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6421ab0-9a96-4cca-b167-089fc9b5f95c",
   "metadata": {},
   "source": [
    "#### Loading LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d316f5b-8f83-42c7-a668-66e9115dbd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.40it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(0) \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained( \n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",  \n",
    "    device_map=\"cuda\",  \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,  \n",
    ") \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468d7da-80d7-4e3b-ba6f-deabd55d3ef0",
   "metadata": {},
   "source": [
    "#### Load the cleaned embeddings from ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fbb65ed-b959-4a2f-aea9-4d244635a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path = \"./data/chromadb/\")\n",
    "msr_ = chroma_client.get_collection(name = \"msr2013-query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653618e-b726-4f77-8f0a-27a66dcfd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"MDB does not contain item\" -> \"\"\n",
    "\n",
    "\"<TOKEN> <TOKEN>\"\n",
    "\"MDB\": \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c25ab819-a835-4698-a5bd-f63787dd7160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are some partial information contained within the database. The provided context mentions issues related to breakpoints, which are not directly related to an endpoint returning error 400. However, the database entries suggest that there might be problems with exception handling or address breakpoints, which could potentially be related to the error.\n",
      "\n",
      "Here are the most relevant entries in the database:\n",
      "\n",
      "- breakpoint in an invalid location (1g4f8p8)\n",
      "- unable to set address breakpoints\n",
      "- exception trying to set breakpoint\n",
      "- cannot add exception breakpoint\n",
      "- unable to create breakpoint\n",
      "- address breakpoints not supported\n",
      "- hardcoded breakpoint (0x80000003), address: 0x23750011\n",
      "- breakpoint location validation error.\n",
      "- breakpoint propertie error.\n",
      "- cannot set exception breakpoint\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def craft_prompt(query: str) -> str:\n",
    "\n",
    "    msr_query = msr_.query(\n",
    "            query_texts=[query],  # Chroma will embed this for you\n",
    "            n_results=10  # How many results to return\n",
    "        )\n",
    "\n",
    "    # Get values that deviate less than 0.1 distance away\n",
    "    documents = np.array(msr_query['documents'][0])\n",
    "    distances = np.array(msr_query['distances'][0])\n",
    "\n",
    "    query_results = documents[distances < (distances[0] + 0.1)].tolist()\n",
    "\n",
    "    # Join results with new lines for the context\n",
    "    context = \"\\n\".join(query_results)\n",
    "\n",
    "    return [ \n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant that is answering questions from a database.\"}, \n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, \n",
    "        respond with \"There are some partial information contained within the database.\" followed by your suggestions.\n",
    "        The context given to you is from a list of possible related defects as found in a database of defect entries.\n",
    "        End off the answer by listing the context that I have provided you with, with the headline, \"Here are the most relevant entries in the database: \"\n",
    "        \n",
    "        >>CONTEXT<<\n",
    "        {context}\\n\n",
    "        >>QUESTION<< {query}\\n\n",
    "        \"\"\"}\n",
    "    ] \n",
    "\n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ") \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"return_full_text\": False, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "output = pipe(craft_prompt(\"endpoint returning error 400\"), **generation_args) \n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325703ea-bc1b-47d5-a419-ff8bd7806c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d03746-f83a-4298-9f95-0aec156ef889",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This\n",
    "# Simple Chat Window, Interactive like ChatGPT\n",
    "# Test Llama Models, Performance and Time Benchmarks\n",
    "\n",
    "\n",
    "### Separate\n",
    "# Technical Documentation PDF separate -> chat on contexts of the pdf\n",
    "# Potentially other topics eg medical; industry specific models\n",
    "# Train LLM ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614ec013-ebf1-4ee7-9bda-96d338d751bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef5b6d5-e4c5-47b3-ad22-499e9f098393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4ee09-1092-4f4d-b312-3eca933c2a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e555659-57c7-474a-972c-7f9da09c3c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0f5b9-b9f3-4a19-9664-bd470d070609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c524f0dd-feaa-41f7-94bd-70d0bff1b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chromadb\n",
    "\n",
    "class LLM:\n",
    "    \n",
    "    def __init__(self,):\n",
    "\n",
    "        # Model Documentation: https://huggingface.co/microsoft/Phi-3-mini-128k-instruct?text=hi+there\n",
    "        \n",
    "        chroma_client = chromadb.PersistentClient(path = \"./data/chromadb/\")\n",
    "        self.msr_ = chroma_client.get_collection(name = \"msr2013-query\")\n",
    "\n",
    "        torch.random.manual_seed(0) \n",
    "        model_id: str = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained( \n",
    "            model_id,  \n",
    "            device_map=\"cuda\",  \n",
    "            torch_dtype=\"auto\",  \n",
    "            trust_remote_code=True,  \n",
    "        ) \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        \n",
    "        self.message = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant that is answering questions from a database.\"}\n",
    "        ]\n",
    "        self.pipe = pipeline( \n",
    "            \"text-generation\", \n",
    "            model=model, \n",
    "            tokenizer=tokenizer, \n",
    "        ) \n",
    "        self.generation_args = { \n",
    "            \"max_new_tokens\": 500, \n",
    "            \"return_full_text\": False, \n",
    "            \"temperature\": 0.0, \n",
    "            \"do_sample\": False, \n",
    "        } \n",
    "\n",
    "        print(\"New Chat Initialised.\")\n",
    "\n",
    "    def _craft_prompt(self, query: str, context: str):\n",
    "        return (\n",
    "            f\"\"\"\n",
    "            Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, \n",
    "            respond with \"There are some partial information contained within the database.\" followed by your suggestions.\n",
    "            The context given to you is from a list of possible related defects as found in a database of defect entries.\n",
    "            End off the answer by listing the context that I have provided you with, with the headline, \"Here are the most relevant entries in the database: \"\n",
    "            \n",
    "            >>CONTEXT<<\n",
    "            {context}\\n\n",
    "            >>QUESTION<< {query}\\n\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    def send_query(self, query: str): # List[Dict[Literal[\"system\", \"user\", \"assistant\"], str]):\n",
    "        \n",
    "        msr_query = self.msr_.query(\n",
    "            query_texts=[query],  # Chroma will embed this for you\n",
    "            n_results=10  # How many results to return\n",
    "        )\n",
    "\n",
    "        # Get values that deviate less than 0.1 distance away\n",
    "        documents = np.array(msr_query['documents'][0])\n",
    "        distances = np.array(msr_query['distances'][0])\n",
    "    \n",
    "        query_results = documents[distances < (distances[0] + 0.1)].tolist()\n",
    "    \n",
    "        # Join results with new lines for the context\n",
    "        context = \"\\n\".join(query_results)\n",
    "        \n",
    "        prompt = self._craft_prompt(query, context)\n",
    "        self.message += [ { \"role\": \"user\", \"content\": query } ]\n",
    "\n",
    "        output = self.pipe(prompt, **self.generation_args) \n",
    "\n",
    "        response = output[0]['generated_text']\n",
    "\n",
    "        self.message += [ { \"role\": \"assistant\", \"content\": response } ]\n",
    "        return response\n",
    "\n",
    "\n",
    "def initiate_chat():\n",
    "    return LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40055a4c-486e-4adb-a162-64aae52aa664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.39it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Chat Initialised.\n"
     ]
    }
   ],
   "source": [
    "llm = initiate_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3af2aa4b-e5c1-4ba2-99fb-2b713766247b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Here are the most relevant entries in the database:\n",
      "            - file: empty files appear as <html><body></body><html>\n",
      "            - file:// url in http pages does not load content\n",
      "            - file:///link on webpage does not work\n",
      "            - page loads improperly when /index.html omitted from url\n",
      "            - crash when opening html code as a local file://\n",
      "\n",
      "            There are some partial information contained within the database.\n",
      "            Suggestions:\n",
      "            - Check if the file path is correct and the file exists.\n",
      "            - Ensure the file is not empty and contains valid HTML code.\n",
      "            - Verify that the server is configured to serve the file correctly.\n",
      "            - Confirm that the URL used to access the file is correct and includes the necessary protocol (file://).\n",
      "            - If the file is part of a larger project, ensure that all dependencies are correctly linked and loaded.\n",
      "            - Check for any server-side errors that might be causing the file to be inaccessible or not found.\n",
      "            - Review the server's error logs for any indications of why the file might be returning a 404 error.\n",
      "            - Ensure that the file permissions are set correctly, allowing the server to read the file.\n",
      "            - If the file is being served from a web server, ensure that the server is configured to handle file requests and that the MIME type is set correctly for HTML files.\n",
      "            - Test the file access in a different browser or on a different device to rule out client-side issues.\n",
      "            - If the file is part of a web application, ensure that the application's routing is correctly configured to serve the file.\n",
      "            - Check for any recent changes in the file or server configuration that might have introduced the issue.\n",
      "            - If the file is being accessed through a web server, ensure that the server is not caching an old version of the file.\n",
      "            - Verify that the file is not being blocked by any security settings or firewalls.\n",
      "            - If the file is part of a content management system (CMS), ensure that the CMS is correctly configured to serve the file.\n",
      "            - Check for any network issues that might be preventing the file from being accessed.\n",
      "            - If the file is being accessed through a proxy server, ensure that the proxy server is correctly\n"
     ]
    }
   ],
   "source": [
    "print(llm.send_query(\"why is my html file closing on 404 on its own?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-defectsearch",
   "language": "python",
   "name": "rapids-defectsearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
