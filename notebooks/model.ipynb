{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be08a58-de10-4648-a19d-af3d5849557e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c272aa3-7bab-49a4-92fc-4abbbf133c38",
   "metadata": {},
   "source": [
    "#### Create the ChromaDB Client and set the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3c02b11-c978-4cb4-9ee5-41a87abf19ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf0e6aae-30dd-493e-af49-cfab618e8527",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMADB_PATH=\"/home/yikuang/workspace/defectsearch/notebooks/data/chromadb\"\n",
    "CHROMADB_COLLECTION_NAME=\"np2024-dataset\"\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMADB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a80e21a-26e5-40dd-9396-d72dba86e1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['np2024-dataset']\n"
     ]
    }
   ],
   "source": [
    "print([entry.name for entry in chroma_client.list_collections()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fcd2c5d-3645-4885-8466-8d9f8dd4b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.get_or_create_collection(\n",
    "    name = CHROMADB_COLLECTION_NAME, \n",
    "    # embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"Huffon/sentence-klue-roberta-base\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae5843-7d6d-45ed-98cd-cea7346697fc",
   "metadata": {},
   "source": [
    "#### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e18aea0-9a1d-4b1b-8f7c-2bc3a66e9b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "files = glob(\"data/github-dataset/warehouse/*.csv\")\n",
    "dfs = [pd.read_csv(file) for file in files]\n",
    "df = dfs[3].dropna(subset = ['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f79ca-5c64-47eb-b01b-d67396fc2a53",
   "metadata": {},
   "source": [
    "#### Clean the data uploaded to ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af1ed142-6b50-49c8-b724-1dba3f9af289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text: str):\n",
    "    pattern = re.compile(r'### Pandas version checks.*?### Reproducible Example', re.DOTALL)\n",
    "    # Replace the matched section with '### Reproducible Example'\n",
    "    cleaned_text = re.sub(pattern, '### Reproducible Example', text)\n",
    "    # Regex to match the ### Installed Versions section and its content\n",
    "    pattern = re.compile(r'### Installed Versions.*?(</details>|$)', re.DOTALL)\n",
    "    # Replace the matched section with an empty string\n",
    "    cleaned_text = re.sub(pattern, '', cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip().lower().replace(\"###\", \"\")\n",
    "    cleaned_text = re.sub(r'\\n\\s*\\n+', ' ', cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59ba1fe3-b907-42ec-bc9a-018e85b86223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1072269/1253343363.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cleaned_body'] = df['body'].apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "df['cleaned_body'] = df['body'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc465050-00fa-4e22-899d-7ffd30e694e4",
   "metadata": {},
   "source": [
    "#### Upload the data to ChromaDB"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eaad2789-a340-4272-8fd2-816e883d5dae",
   "metadata": {},
   "source": [
    "import uuid\n",
    "from tqdm import tqdm\n",
    "\n",
    "i, chunk_size = 0, 1_000\n",
    "\n",
    "for row in df.itertuples():\n",
    "    # Insert the embedded sentences into the database on every chunk_size chunk\n",
    "    if i % chunk_size == 0 and i != 0:        \n",
    "        chunk_df = df.iloc[i - chunk_size: i]\n",
    "        collection.upsert(\n",
    "            documents = chunk_df[\"body\"].to_list(), \n",
    "            ids = chunk_df[\"node_id\"].to_list()\n",
    "        )\n",
    "    i += 1\n",
    "    \n",
    "collection.upsert(\n",
    "    documents = df.iloc[i - chunk_size:]['body'].to_list(), \n",
    "    ids = df.iloc[i - chunk_size:][\"node_id\"].to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45dbe08f-959c-483b-a353-efadfb956c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"pd.ExcelWriter cannot accept an io.BytesIO instance as first arg\"\n",
    "results = collection.query(query_texts=[query], n_results=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c83da1-2580-44c7-b570-4b6d04332532",
   "metadata": {},
   "source": [
    "#### Import the Pretrained LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02bb7f80-8d31-4475-be17-22f210d31cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████████████████| 2/2 [00:01<00:00,  1.74it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import numpy as np\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b76f791a-cacd-4633-a93b-09fc0c01813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def craft_prompt(query: str, df: pd.DataFrame, chroma_collection, min_ref_docs: int = 2, distance_threshold = 0.1) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    min_ref_docs : minimum number of documents we want the RAG model to reference.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = chroma_collection.query(\n",
    "        query_texts = [query],  # Chroma will embed this for you\n",
    "        n_results = 10,  # How many results to return\n",
    "    )\n",
    "\n",
    "    reference_ids: List[str] = results['ids'][0]\n",
    "    filtered_result = df[df.node_id.isin(reference_ids)]\n",
    "\n",
    "    # Get values that deviate less than 0.1 distance away\n",
    "    documents = np.array(filtered_result['cleaned_body'].to_list())\n",
    "    distances = np.array(results[\"distances\"][0])\n",
    "\n",
    "    distance_ids = {k: v for k,v in zip(reference_ids, distances)}\n",
    "    \n",
    "    relevant_documents = documents[distances < min(distances) + distance_threshold]\n",
    "\n",
    "    if len(relevant_documents) < min_ref_docs:\n",
    "        relevant_documents = documents[:min_ref_docs]\n",
    "    \n",
    "    if len(relevant_documents) == 0:\n",
    "        relevant_documents = documents[:1]  # At least take the top result if none within the threshold\n",
    "\n",
    "    # Join results with new lines for the context\n",
    "    context = \"\\n\".join(relevant_documents) \n",
    "\n",
    "    return relevant_documents\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant that answers questions from a database of GitHub issues.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Answer the following question using only the provided context. Do not assume or add information beyond what is in the context. \\n\"\n",
    "                \"If the context does not contain sufficient information to answer the question, explicitly state that the context is insufficient and provide your own suggestions with a clear warning. \\n\"\n",
    "                \"The context are entries in a database, so your answer should say instead that you had referenced the database. \\n\"\n",
    "                \"Context:\\n\"\n",
    "                \"{context}\\n\\n\"\n",
    "                \"Question: {query}\\n\\n\"\n",
    "                \"Here are the entries in the database that might be relevant to your query:\\n\"\n",
    "                \"{entries}\"\n",
    "            ).format(context=context, query=query, entries=\"\".join([f'- {entry}\\n' for entry in relevant_documents]))\n",
    "        }\n",
    "    ], distance_ids\n",
    "\n",
    "def search_term(query, pipe, df, chroma_collection):\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "\n",
    "    prompt = craft_prompt(query, df, chroma_collection)\n",
    "\n",
    "    return prompt\n",
    "    \n",
    "    output = pipe(prompt, **generation_args)\n",
    "    \n",
    "    return output, reference_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3197195-b036-4d53-978b-1e3acab469f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Ensure 'collection' is properly initialized and configured\n",
    "query = \"test\"\n",
    "\n",
    "relevant_documents = search_term(\n",
    "    query,\n",
    "    pipe, \n",
    "    df, \n",
    "    chroma_collection = collection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c6179e7-3408-4b9f-9b5d-90076af1d0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.9792,  -7.9862, -11.0184, -11.0404, -11.0475])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-m3')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-v2-m3')\n",
    "model.eval()\n",
    "\n",
    "# # Define pairs for similarity computation\n",
    "# query = 'china has bears?'\n",
    "# passages = [\n",
    "#     'hi',\n",
    "#     'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'\n",
    "# ]\n",
    "\n",
    "# Prepare input pairs\n",
    "pairs = [[query, passage] for passage in relevant_documents]\n",
    "\n",
    "# Tokenize and compute scores\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    outputs = model(**inputs, return_dict=True)\n",
    "    scores = outputs.logits.squeeze().float()  # Adjust based on model output shape\n",
    "    print(scores)\n",
    "\n",
    "# Pair the scores with the other list\n",
    "paired_list = list(zip(scores, relevant_documents))\n",
    "# Sort the pairs based on the scores in descending order\n",
    "sorted_pairs = sorted(paired_list, key=lambda x: x[0], reverse=True)\n",
    "# Separate the sorted pairs back into two lists\n",
    "sorted_scores, sorted_other_list = zip(*sorted_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306a5ad-c526-4347-81b0-c8c6981c06ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-search",
   "language": "python",
   "name": "semantic-search"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
