{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a42c120-c00e-4598-a15b-9914243c9abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yikuang/miniconda3/envs/rapids-defectsearch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import transformers\n",
    "from time import time\n",
    "from typing import List\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "# Llama model takes about 16s per query\n",
    "hf_token = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6421ab0-9a96-4cca-b167-089fc9b5f95c",
   "metadata": {},
   "source": [
    "#### Loading LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d316f5b-8f83-42c7-a668-66e9115dbd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.52it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(0) \n",
    "\n",
    "model_id = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained( \n",
    "    model_id,  \n",
    "    device_map=\"cuda\",  \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,  \n",
    "    token = hf_token\n",
    ") \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, force_download = True, token = hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468d7da-80d7-4e3b-ba6f-deabd55d3ef0",
   "metadata": {},
   "source": [
    "#### Load the cleaned embeddings from ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fbb65ed-b959-4a2f-aea9-4d244635a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path = \"../mlengine/data/chromadb\")\n",
    "msr_ = chroma_client.get_collection(name = \"msr2013-query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c25ab819-a835-4698-a5bd-f63787dd7160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yikuang/miniconda3/envs/rapids-defectsearch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are some partial information contained within the database. The provided context indicates an HTTP status 500 error, which is commonly associated with internal server errors. However, without more specific details about the error, it's challenging to pinpoint the exact cause. To better understand the issue, consider checking server logs, application error messages, or consulting with the development team responsible for the server or application.\n",
      "\n",
      "Here are the most relevant entries in the database:\n",
      "\n",
      "- http status 500 error\n",
      "- Internal Server Error\n",
      "- Server-side error\n",
      "- Application error\n"
     ]
    }
   ],
   "source": [
    "def craft_prompt(query: str) -> str:\n",
    "\n",
    "    msr_query = msr_.query(\n",
    "            query_texts=[query],  # Chroma will embed this for you\n",
    "            n_results=10  # How many results to return\n",
    "        )\n",
    "\n",
    "    # Get values that deviate less than 0.1 distance away\n",
    "    documents = np.array(msr_query['documents'][0])\n",
    "    distances = np.array(msr_query['distances'][0])\n",
    "\n",
    "    query_results = documents[distances < (distances[0] + 0.1)].tolist()\n",
    "\n",
    "    # Join results with new lines for the context\n",
    "    context = \"\\n\".join(query_results)\n",
    "\n",
    "    return [ \n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant that is answering questions from a database.\"}, \n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, \n",
    "        respond with \"There are some partial information contained within the database.\" followed by your suggestions.\n",
    "        The context given to you is from a list of possible related defects as found in a database of defect entries.\n",
    "        End off the answer by listing the context that I have provided you with, with the headline, \"Here are the most relevant entries in the database: \"\n",
    "        \n",
    "        >>CONTEXT<<\n",
    "        {context}\\n\n",
    "        >>QUESTION<< {query}\\n\n",
    "        \"\"\"}\n",
    "    ] \n",
    "\n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ") \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"return_full_text\": False, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "output = pipe(craft_prompt(\"error 500 on internal server error\"), **generation_args) \n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d03746-f83a-4298-9f95-0aec156ef889",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This\n",
    "# Simple Chat Window, Interactive like ChatGPT\n",
    "# Test Llama Models, Performance and Time Benchmarks\n",
    "\n",
    "\n",
    "### Separate\n",
    "# Technical Documentation PDF separate -> chat on contexts of the pdf\n",
    "# Potentially other topics eg medical; industry specific models\n",
    "# Train LLM ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c524f0dd-feaa-41f7-94bd-70d0bff1b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chromadb\n",
    "\n",
    "class LLM:\n",
    "    \n",
    "    def __init__(self,):\n",
    "\n",
    "        # Model Documentation: https://huggingface.co/microsoft/Phi-3-mini-128k-instruct?text=hi+there\n",
    "        \n",
    "        chroma_client = chromadb.PersistentClient(path = \"./data/chromadb/\")\n",
    "        self.msr_ = chroma_client.get_collection(name = \"msr2013-query\")\n",
    "\n",
    "        torch.random.manual_seed(0) \n",
    "        model_id: str = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained( \n",
    "            model_id,  \n",
    "            device_map=\"cuda\",  \n",
    "            torch_dtype=\"auto\",  \n",
    "            trust_remote_code=True,  \n",
    "        ) \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        \n",
    "        self.message = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant that is answering questions from a database.\"}\n",
    "        ]\n",
    "        self.pipe = pipeline( \n",
    "            \"text-generation\", \n",
    "            model=model, \n",
    "            tokenizer=tokenizer, \n",
    "        ) \n",
    "        self.generation_args = { \n",
    "            \"max_new_tokens\": 500, \n",
    "            \"return_full_text\": False, \n",
    "            \"temperature\": 0.0, \n",
    "            \"do_sample\": False, \n",
    "        } \n",
    "\n",
    "        print(\"New Chat Initialised.\")\n",
    "\n",
    "    def _craft_prompt(self, query: str, context: str):\n",
    "        return (\n",
    "            f\"\"\"\n",
    "            Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, \n",
    "            respond with \"There are some partial information contained within the database.\" followed by your suggestions.\n",
    "            The context given to you is from a list of possible related defects as found in a database of defect entries.\n",
    "            End off the answer by listing the context that I have provided you with, with the headline, \"Here are the most relevant entries in the database: \"\n",
    "            \n",
    "            >>CONTEXT<<\n",
    "            {context}\\n\n",
    "            >>QUESTION<< {query}\\n\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    def send_query(self, query: str): # List[Dict[Literal[\"system\", \"user\", \"assistant\"], str]):\n",
    "        \n",
    "        msr_query = self.msr_.query(\n",
    "            query_texts=[query],  # Chroma will embed this for you\n",
    "            n_results=10  # How many results to return\n",
    "        )\n",
    "\n",
    "        # Get values that deviate less than 0.1 distance away\n",
    "        documents = np.array(msr_query['documents'][0])\n",
    "        distances = np.array(msr_query['distances'][0])\n",
    "    \n",
    "        query_results = documents[distances < (distances[0] + 0.1)].tolist()\n",
    "    \n",
    "        # Join results with new lines for the context\n",
    "        context = \"\\n\".join(query_results)\n",
    "        \n",
    "        prompt = self._craft_prompt(query, context)\n",
    "        self.message += [ { \"role\": \"user\", \"content\": query } ]\n",
    "\n",
    "        output = self.pipe(prompt, **self.generation_args) \n",
    "\n",
    "        response = output[0]['generated_text']\n",
    "\n",
    "        self.message += [ { \"role\": \"assistant\", \"content\": response } ]\n",
    "        return response\n",
    "\n",
    "\n",
    "def initiate_chat():\n",
    "    return LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40055a4c-486e-4adb-a162-64aae52aa664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.39it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Chat Initialised.\n"
     ]
    }
   ],
   "source": [
    "llm = initiate_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3af2aa4b-e5c1-4ba2-99fb-2b713766247b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Here are the most relevant entries in the database:\n",
      "            - file: empty files appear as <html><body></body><html>\n",
      "            - file:// url in http pages does not load content\n",
      "            - file:///link on webpage does not work\n",
      "            - page loads improperly when /index.html omitted from url\n",
      "            - crash when opening html code as a local file://\n",
      "\n",
      "            There are some partial information contained within the database.\n",
      "            Suggestions:\n",
      "            - Check if the file path is correct and the file exists.\n",
      "            - Ensure the file is not empty and contains valid HTML code.\n",
      "            - Verify that the server is configured to serve the file correctly.\n",
      "            - Confirm that the URL used to access the file is correct and includes the necessary protocol (file://).\n",
      "            - If the file is part of a larger project, ensure that all dependencies are correctly linked and loaded.\n",
      "            - Check for any server-side errors that might be causing the file to be inaccessible or not found.\n",
      "            - Review the server's error logs for any indications of why the file might be returning a 404 error.\n",
      "            - Ensure that the file permissions are set correctly, allowing the server to read the file.\n",
      "            - If the file is being served from a web server, ensure that the server is configured to handle file requests and that the MIME type is set correctly for HTML files.\n",
      "            - Test the file access in a different browser or on a different device to rule out client-side issues.\n",
      "            - If the file is part of a web application, ensure that the application's routing is correctly configured to serve the file.\n",
      "            - Check for any recent changes in the file or server configuration that might have introduced the issue.\n",
      "            - If the file is being accessed through a web server, ensure that the server is not caching an old version of the file.\n",
      "            - Verify that the file is not being blocked by any security settings or firewalls.\n",
      "            - If the file is part of a content management system (CMS), ensure that the CMS is correctly configured to serve the file.\n",
      "            - Check for any network issues that might be preventing the file from being accessed.\n",
      "            - If the file is being accessed through a proxy server, ensure that the proxy server is correctly\n"
     ]
    }
   ],
   "source": [
    "print(llm.send_query(\"why is my html file closing on 404 on its own?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-defectsearch",
   "language": "python",
   "name": "rapids-defectsearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
